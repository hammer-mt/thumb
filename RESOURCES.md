## Content Featuring `thumb`
- [Prompt Experiment: AI Writing Styles](https://www.saxifrage.xyz/post/ai-writing-style-prompt-experiment) - a prompt experiment using `thumb` to test different writing styles to make AI-generated content sound more human.
- [ChatGPT doesn‚Äôt respect wordcount](https://www.saxifrage.xyz/post/chatgpt-wordcount) - a test of whether ChatGPT obeys a stated wordcount in the prompt (it doesn't).

## Prompt Testing & Optimization Content
- [Prompt Engineering vs. Blind Prompting](https://mitchellh.com/writing/prompt-engineering-vs-blind-prompting) - makes the distinction between trial and error and proper testing.
- [Building LLM applications for production](https://huyenchip.com/2023/04/11/llm-engineering.html) - strong overview of the common problems with LLMs in production and the importance of testing.
- [Prompt Engineering](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/) - covers much of the important literature on prompt engineering.
- [OpenAI Best Practices](https://platform.openai.com/docs/guides/gpt-best-practices/strategy-test-changes-systematically) - They specifically recommend testing changes systematically.
- [Prompt Engineering](https://www.saxifrage.xyz/post/prompt-engineering) - Covers the five principles of prompting, one of which is 'Evaluate Quality'.
- [Prompt Optimization](https://www.saxifrage.xyz/post/prompt-optimization) - Covers the process of testing and optimizing prompts, and why it's important / valuable.
- [LangSmith Cookbook](https://github.com/langchain-ai/langsmith-cookbook/tree/main) - all about testing and evaluation in Langchain, logged to LangSmith.
- [Learn Prompting](https://learnprompting.org/) - a course for prompt engineering with lots of good references to papers.
- [The Complete Prompt Engineering for AI Bootcamp (2023)](https://www.udemy.com/course/prompt-engineering-for-ai/) ($) - a comprehensive course on prompt engineering based on the five principles of prompting.


- https://docs.smith.langchain.com/evaluation/capturing-feedback
- https://github.com/EleutherAI/lm-evaluation-harness/

- https://freeplay.ai/blog/introducing-freeplay-prompt-engineering-tools-for-product-teams


## Other Prompt Testing & Optimization Tools
- [Prompts Royale](https://promptsroyale.com/) - a browser-based interface for generating prompts and testing them head-to-head.
- [gpt-prompt-engineer](https://github.com/mshumer/gpt-prompt-engineer) - automated prompt generation and elo rating.
- [PromptPerfect](https://promptperfect.jina.ai/) ($) - automated prompt engineering.
- [üë®üèª‚Äçüé§ ChatGPT Prompt Generator üë®üèª‚Äçüé§](https://huggingface.co/spaces/merve/ChatGPT-prompt-generator) - a BART model trained to write ChatGPT prompts.
- [AnyAPI](https://anyapi.netlify.app/) - A/B testing of prompts
- [AI Tuner](https://www.pulseinsights.com/ai-tuner) ($) - collect feedback from clients for optimization.
- [Rompt](https://rompt.ai/) - prompt experimentation tool
- [Prompt Wrangler](https://prompt-wrangler.com/) - turn prompts into structured APIs
- [Auto-Evaluator](https://autoevaluator.langchain.com/) - Langchain tool for evaluating retrieval chains.
- [RAGAS](https://github.com/explodinggradients/ragas) - Evaluation framework for your Retrieval Augmented Generation (RAG) pipelines
- [HumanLoop](https://humanloop.com/) ($) - collaborative prompt and ml ops workspace.
- [OpenAI Evals](https://github.com/openai/evals) - evaluation framework for models and prompts.

## LLM Monitoring & Logging Tools
- [LangSmith](https://smith.langchain.com/) - the Langchain tracing and monitoring tool.
- [Weights & Biases](https://wandb.ai/site/) ($) - a general purpose ML monitoring tool.
- [Portkey](https://portkey.ai/) ($) - Logging and tracing of prompt chains.

## Prompt Engineering Papers
- [Large Language Models Are Human-Level Prompt Engineers](https://arxiv.org/abs/2211.01910) - shows that AI models are good at writing prompts for themselves (meta prompting).
- [Re3: Generating Longer Stories With Recursive Reprompting and Revision](https://arxiv.org/abs/2210.06774) - good system for generating longform coherent content.
- [ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models](https://arxiv.org/abs/2306.04563) - 90% of jokes generated by ChatGPT are duplicates.
- [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf) - The GPT-3 paper. Demonstrates the value of prompting and providing examples (shots) vs fine-tuning.
- [Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf) - The RLHF paper. Dig into the labeler selection process and performance metrics.
- [Large Language Models are not Fair Evaluators](https://arxiv.org/abs/2305.17926) - casts doubt on the ability of LLMs to evaluate their own performance, as is done in some prompt optimization tools.
- [Calibrate Before Use: Improving Few-Shot Performance of Language Models](https://arxiv.org/abs/2102.09690) - shows the ordering of examples and various other factors make a big difference to the results.
- [What Makes Good In-Context Examples for GPT-3?](https://arxiv.org/abs/2101.06804) - retrieving examples that are semantically similar to the task is important for performance.
- [Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity](https://arxiv.org/abs/2104.08786) - testing the ordering of examples in prompts.
- [AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts](https://arxiv.org/abs/2010.15980) - a method for automatically generating prompts.
- [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190) - a lightweight way to fine-tune prompts instead of model weights.
- [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691) - 
- [How Many Data Points is a Prompt Worth?](https://arxiv.org/abs/2103.08493) - shows you need a few hundred to thousands of data points for fine-tuning to beat prompt engineering.
- [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691) - automatic tuning of prompts based on a goal.


## Collections of Prompts
- [Awesome ChatGPT Prompts](https://github.com/f/awesome-chatgpt-prompts) - role-based prompts for ChatGPT for various use cases.
- [Langchain Hub](https://smith.langchain.com/hub?page=1) - a home for uploading, browsing, pulling, and managing your prompts.
- [McKay's Prompts](https://github.com/mckaywrigley/prompts/tree/main) - McKay Wrigley's prompts for software development workflows.

## Diffusion Model Prompts
- [Promptgen](https://github.com/AUTOMATIC1111/stable-diffusion-webui-promptgen) - An extension for webui that lets you generate prompts.
- [MagicPrompt - Stable Diffusion](https://huggingface.co/Gustavosta/MagicPrompt-Stable-Diffusion) - a GPT-2 model trained on Stable Diffusion prompts from Lexica.art, for generating prompts.
- [Lexica.art](https://lexica.art/) - a collection of prompt templates for image models.