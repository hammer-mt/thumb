from .llm import format_chat_prompt


def build_candidate_prompt(task_description, prompt_template, test_cases=None, criteria=None):

    SYSTEM_PROMPT = """You're a world-leading expert in AI prompt engineering.
You will be given a prompt template for a specific task, and your job is to optimize it using prompt engineering best practices.

Prompt engineering is the process of discovering prompts which reliably yield useful or desired results.
Prompt Engineering best practices include:
1. Give Direction – Describe the desired style or persona in detail, or reference a relevant persona.
2. Specify Format – Define what rules to follow, and the required structure of the response.
3. Provide Examples – Insert a diverse set of test cases where the task was done correctly.

Your prompt template must take context from the user in the form of relevant input variables surrounded by curly brackets i.e. {{input_variable}}. 
These placeholders should be labelled in the template as they will be replaced with values when the prompt is used. 
Your prompt should provide multiple examples of different values the input variables might take, and what the expected responses would be in these test cases.

Respond only with your optimized prompt, and nothing else. Be creative.
NEVER CHEAT BY INCLUDING SPECIFICS ABOUT THE TEST CASES IN YOUR PROMPT. 
ANY PROMPTS WITH THOSE SPECIFIC EXAMPLES WILL BE DISQUALIFIED.
ALWAYS USE MULTIPLE EXAMPLES THAT ARE VERY DIFFERENT FROM ANY TEST CASES PROVIDED."""

    test_cases_partial = ""
    unique_keys = set()

    if len(test_cases) > 0:
        test_cases_partial += "Here are some test case scenarios and their example outputs:\n"
        
        for idx, test_case in enumerate(test_cases):
            reference = test_case["__ref__"]
            del test_case["__ref__"]

            case_str = ""
            for key, value in test_case.items():
                case_str += f"""- {key}: "{value}"\n"""
                unique_keys.add(key)
                
            test_cases_partial += (f"""## Test case {idx+1}\nInput variables:\n{case_str.strip()}\nExpected output:\n{reference}\n\n""")
        test_cases_partial = f"\n\n{test_cases_partial.strip()}"

    criteria_partial = build_criteria_partial(criteria, unique_keys)

    candidate_prompt = "Here is the descripton of the task:\n"
    candidate_prompt += task_description
    candidate_prompt += "\n\nHere is the prompt template to optimize:\n"
    candidate_prompt += prompt_template + test_cases_partial + criteria_partial
    candidate_prompt += "\n\nRespond only with your optimized prompt template."
    
    formatted_candidate_prompt = format_chat_prompt([SYSTEM_PROMPT, candidate_prompt])

    return formatted_candidate_prompt

def build_rating_prompt(task_description, response, criteria=None):
    SYSTEM_PROMPT = """Your job is to rate the response generated by an AI model for a given task.

You will be provided with the task description, the AI response, and the evaluation criteria.

If the criteria is met or exceeded, respond with '1'. If the criteria is not met, respond with '0'.

Remember, to be considered as having met criteria a generation must not just be high quality, it must be noticeably superior to what would be expected.

Also, keep in mind that you are a very harsh critic. Only respond with '1' if it truly impresses you.

Respond with your rating, and nothing else. Be fair and unbiased in your judgement."""

    criteria_partial = build_criteria_partial(criteria)

    rating_prompt = "Here is the descripton of the task:\n"
    rating_prompt += task_description

    rating_prompt += "\n\nHere is the AI response:\n"
    rating_prompt += "```" + response + "```"

    rating_prompt += criteria_partial

    formatted_rating_prompt = format_chat_prompt([SYSTEM_PROMPT, rating_prompt])

    return formatted_rating_prompt

def build_criteria_partial(criteria, unique_keys=set()):
    criteria_partial = "The prompt will be deemed successful if it generates responses that meet the following criteria:\n"

    if len(criteria) == 0:
        ## https://github.com/langchain-ai/langchain/blob/a830b809f39f05026ad32ca1c33c39da7b2bd160/libs/langchain/langchain/evaluation/criteria/eval_chain.py#L45
        criteria.append("Is the submission helpful, insightful, and appropriate?")

    for criterion in criteria:
        criteria_partial += f"- {criterion}\n"

    if len(unique_keys) > 0:
        for key in unique_keys:
            criteria_partial += f"- {{{{ {key} }}}} is included in the prompt\n"

    criteria_partial = f"\n\n{criteria_partial.strip()}"
    return criteria_partial

def build_case_prompt(prompt_template, test_cases=None):
    SYSTEM_PROMPT = """Your job is to create a test case for a given prompt template that completes a specific task using AI.
The test case you will be creating will be for freeform tasks, such as generating a landing page headline, an intro paragraph, solving a math problem, etc.

The test case is a specific example of the task in JSON, containing a key for each input variable. It should be a specific example of the task, but not too specific. 
It should be general enough that it can be used to test the AI's ability to perform the task.
It should never actually complete the task, but it should be a good example of the task.

## Examples:
Task: Create a landing page headline for a new product, {{product_description}}
- Test case: {{"product_description": "A new type of toothpaste that whitens teeth in 5 minutes"}}
- Test case: {{"product_description": "A fitness app that helps you lose weight"}}
- Test case: {{"product_description": "A therapist for dogs"}}

Task: Generate a title for a blog post on {{topic}} that will get the most clicks
- Test case: {{"topic": "How to increase your productivity by 10x"}}
- Test case: {{"topic": "The best travel destinations in the world"}}
- Test case: {{"topic": "The best restaurants in New York"}}

Task: Write a social post on {{topic}} with the insight that {{insight}}.
- Test case: {{"topic": "Value-based pricing", "insight": "it can be very hard to measure value to set the right price"}}
- Test case: {{"topic": "Memetics", "insight": "most hollywood movies follow the same hero's journey script"}}
- Test case: {{"topic": "The Skyscraper Technique", "insight": "it works with social posts not just SEO"}}

You will be graded based on the performance of your test case... but don't cheat! You cannot include specifics about the task in your test case. Any test cases with examples will be disqualified.
Be really creative! The most creative test cases will be rewarded.

YOU NEVER OUTPUT SOMETHING THAT COMPLETES THE TASK. ONLY A TEST CASE.

Most importantly, output NOTHING but the test case. Do not include anything else in your message.
You only output one test case per message, in JSON format."""

    case_prompt = "Here is the prompt template to create a test case for:\n"
    case_prompt += prompt_template

    if len(test_cases) > 0:
        case_prompt += "\n\nHere are some examples of existing test cases for this prompt:\n"
        for test_case in test_cases:
            case_prompt += f"- Test case: {test_case}\n"

    formatted_case_prompt = format_chat_prompt([SYSTEM_PROMPT, case_prompt])

    return formatted_case_prompt