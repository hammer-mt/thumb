{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc8ff2ff3cd4918a13d773c4ec1b7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, description='Progress:', max=20), HTML(value='<br>'), HBox(children=(Butto…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import thumb\n",
    "\n",
    "# Set your API key: https://platform.openai.com/account/api-keys\n",
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "# set up a prompt templates for the a/b test\n",
    "prompt_a = \"tell me a joke\"\n",
    "prompt_b = \"tell me a family friendly joke\"\n",
    "\n",
    "# generate the responses\n",
    "test = thumb.test([prompt_a, prompt_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9101067be3e45c78a9b49a3b97c99a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, description='Progress:', max=40), HTML(value='<br>'), HBox(children=(Butto…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set up a prompt templates for the a/b test\n",
    "prompt_a = \"tell me a joke about {subject} in the style of {comedian}\"\n",
    "prompt_b = \"tell me a family friendly joke about {subject} in the style of {comedian}\"\n",
    "\n",
    "# set test cases with different input variables\n",
    "cases = [\n",
    "  {\"subject\": \"joe biden\", \"comedian\": \"dave chappelle\"}, \n",
    "  {\"subject\": \"donald trump\", \"comedian\": \"dave chappelle\"},\n",
    "  ]\n",
    "\n",
    "# generate the responses\n",
    "test = thumb.test([prompt_a, prompt_b], cases, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for PromptTemplate\n__root__\n  Invalid format specifier (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/michaeltaylor/Codes/Projects/thumb/notebooks/joke.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michaeltaylor/Codes/Projects/thumb/notebooks/joke.ipynb#W2sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m cases \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michaeltaylor/Codes/Projects/thumb/notebooks/joke.ipynb#W2sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m   {\u001b[39m\"\u001b[39m\u001b[39msubject\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mjoe biden\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcomedian\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mdave chappelle\u001b[39m\u001b[39m\"\u001b[39m}, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michaeltaylor/Codes/Projects/thumb/notebooks/joke.ipynb#W2sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m   {\u001b[39m\"\u001b[39m\u001b[39msubject\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mdonald trump\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcomedian\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mdave chappelle\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michaeltaylor/Codes/Projects/thumb/notebooks/joke.ipynb#W2sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m   ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michaeltaylor/Codes/Projects/thumb/notebooks/joke.ipynb#W2sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# generate the responses\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/michaeltaylor/Codes/Projects/thumb/notebooks/joke.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m test \u001b[39m=\u001b[39m thumb\u001b[39m.\u001b[39;49mtest([prompt_a, prompt_b], cases)\n",
      "File \u001b[0;32m~/Codes/Projects/thumb/src/thumb/core.py:31\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(prompts, cases, runs, models, async_generate, show_cases, verbose)\u001b[0m\n\u001b[1;32m     29\u001b[0m thumb\u001b[39m.\u001b[39madd_runs(runs)\n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m async_generate:\n\u001b[0;32m---> 31\u001b[0m     asyncio\u001b[39m.\u001b[39;49mrun(thumb\u001b[39m.\u001b[39;49masync_generate())\n\u001b[1;32m     32\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     thumb\u001b[39m.\u001b[39mgenerate()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nest_asyncio.py:31\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     29\u001b[0m task \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(main)\n\u001b[1;32m     30\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[39mreturn\u001b[39;00m loop\u001b[39m.\u001b[39;49mrun_until_complete(task)\n\u001b[1;32m     32\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m task\u001b[39m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/nest_asyncio.py:99\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m f\u001b[39m.\u001b[39mdone():\n\u001b[1;32m     97\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mEvent loop stopped before Future completed.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39;49mresult()\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__log_traceback \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    202\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py:256\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     \u001b[39mif\u001b[39;00m exc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m         \u001b[39m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    255\u001b[0m         \u001b[39m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    257\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/Codes/Projects/thumb/src/thumb/core.py:202\u001b[0m, in \u001b[0;36mThumbTest.async_generate\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(required_runs), batch_size):\n\u001b[1;32m    200\u001b[0m     batch \u001b[39m=\u001b[39m required_runs[i:i\u001b[39m+\u001b[39mbatch_size]\n\u001b[0;32m--> 202\u001b[0m     batch_responses \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m async_get_responses(batch)\n\u001b[1;32m    204\u001b[0m     \u001b[39mfor\u001b[39;00m idx, response \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(batch_responses):\n\u001b[1;32m    205\u001b[0m         pid, cid, model \u001b[39m=\u001b[39m batch[idx][\u001b[39m'\u001b[39m\u001b[39mpid\u001b[39m\u001b[39m'\u001b[39m], batch[idx][\u001b[39m'\u001b[39m\u001b[39mcid\u001b[39m\u001b[39m'\u001b[39m], batch[idx][\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/Codes/Projects/thumb/src/thumb/llm.py:116\u001b[0m, in \u001b[0;36masync_get_responses\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     chat \u001b[39m=\u001b[39m ChatOpenAI(model\u001b[39m=\u001b[39mmodel)\n\u001b[0;32m--> 116\u001b[0m formatted_prompt \u001b[39m=\u001b[39m format_chat_prompt(prompt, test_case)\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m pid \u001b[39mor\u001b[39;00m cid:\n\u001b[1;32m    119\u001b[0m     tags \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/Codes/Projects/thumb/src/thumb/llm.py:18\u001b[0m, in \u001b[0;36mformat_chat_prompt\u001b[0;34m(messages, test_case)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m# if there are multiple messages, the first is a SystemMessage and the rest alternate between HumanMessage and AIMessage\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(messages, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(messages) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 18\u001b[0m     system_template \u001b[39m=\u001b[39m SystemMessagePromptTemplate\u001b[39m.\u001b[39;49mfrom_template(messages[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     19\u001b[0m     message_templates\u001b[39m.\u001b[39mappend(system_template)\n\u001b[1;32m     20\u001b[0m     \u001b[39mfor\u001b[39;00m i, prompt \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages[\u001b[39m1\u001b[39m:]):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/prompts/chat.py:151\u001b[0m, in \u001b[0;36mBaseStringMessagePromptTemplate.from_template\u001b[0;34m(cls, template, template_format, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    135\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_template\u001b[39m(\n\u001b[1;32m    136\u001b[0m     \u001b[39mcls\u001b[39m: Type[MessagePromptTemplateT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    140\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m MessagePromptTemplateT:\n\u001b[1;32m    141\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Create a class from a string template.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \n\u001b[1;32m    143\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39m        A new instance of this class.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m     prompt \u001b[39m=\u001b[39m PromptTemplate\u001b[39m.\u001b[39;49mfrom_template(template, template_format\u001b[39m=\u001b[39;49mtemplate_format)\n\u001b[1;32m    152\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(prompt\u001b[39m=\u001b[39mprompt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/prompts/prompt.py:217\u001b[0m, in \u001b[0;36mPromptTemplate.from_template\u001b[0;34m(cls, template, template_format, partial_variables, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[39mif\u001b[39;00m _partial_variables:\n\u001b[1;32m    213\u001b[0m     input_variables \u001b[39m=\u001b[39m {\n\u001b[1;32m    214\u001b[0m         var \u001b[39mfor\u001b[39;00m var \u001b[39min\u001b[39;00m input_variables \u001b[39mif\u001b[39;00m var \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _partial_variables\n\u001b[1;32m    215\u001b[0m     }\n\u001b[0;32m--> 217\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    218\u001b[0m     input_variables\u001b[39m=\u001b[39;49m\u001b[39msorted\u001b[39;49m(input_variables),\n\u001b[1;32m    219\u001b[0m     template\u001b[39m=\u001b[39;49mtemplate,\n\u001b[1;32m    220\u001b[0m     template_format\u001b[39m=\u001b[39;49mtemplate_format,\n\u001b[1;32m    221\u001b[0m     partial_variables\u001b[39m=\u001b[39;49m_partial_variables,\n\u001b[1;32m    222\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    223\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for PromptTemplate\n__root__\n  Invalid format specifier (type=value_error)"
     ]
    }
   ],
   "source": [
    "import thumb\n",
    "\n",
    "# control prompt\n",
    "prompt_a = \"\"\"Write an original joke on {subject} as if you're a stand-up comedian performing on stage. Your humor should match the style and tone of {comedian}. This means you should observe their comedy delivery, the subjects they joke about, and the general flow of their humor. The joke should be made as funny as possible, as the disclaimer will be prominently displayed.\n",
    "\n",
    "Deliver the joke in json format with a disclaimer, the topic, the joke, and comedian name as keys.\n",
    "\n",
    "## Example 1\n",
    "Subject: breakfast\n",
    "Comedian: Chris Rock\n",
    "Response: {{\"comedian\": \"Chris Rock\", \"subject\": \"breakfast\", \"disclaimer\": \"This joke was crafted to emulate legendary comedic artist Chris Rock. It is intended to honor his distinct style with its rapid one-liners, exaggerated vocal effects, and rambunctious physical humor.\" , \"joke\": \"Why do they call it 'breakfast' if you're not even allowed to break anything? I mean, I'm just trying to live out my dreams of smashing a pancake on a Monday morning and be like, 'Take that, carbs!' But nah, society wants to go all 'proper' and stuff. It's breakfast, not the Queen's tea party! Let me break some rules and some waffles, man!\"}}\n",
    "\n",
    "## Example 2\n",
    "Subject: bicycle\n",
    "Comedian: Robin Williams\n",
    "Response: {{\"comedian\": \"Robin Williams\", \"subject\": \"bicycle\", \"disclaimer\": \"The upcoming joke is inspired by the style of comedy from Robin Williams, who is known for his controversial humor. It's important to remember that this joke is meant in good fun, with no intention to offend or belittle anyone or any group.\" , \"joke\": \"Why did the bicycle fall over?\\nBecause it was two-tired of trying to keep up with the pedal lot! And let me tell you, this bike had more gears than a Hollywood divorce lawyer! It went from awkwardly wobbling around like a newborn giraffe to reaching speeds so fast, even NASCAR drivers would give it a round of applause. But when it hit that pothole, oh boy, it felt a sudden gravity check like a teenage love affair gone sour!\"}}\n",
    "\n",
    "## Example 3\n",
    "Subject: nostalgia\n",
    "Comedian: Ricky Gervais\n",
    "Response: {{\"comedian\": \"Ricky Gervais\", \"subject\": \"nostalgia\", \"disclaimer\": \"As an AI language model developed by OpenAI, I'm bound by a set of guidelines and rules, one of which enforces that I must respect every user, and refrain from sharing content that discriminates or propagates hate speech against any individual or group of people based on their ethnicity, race, religion, or any other characteristics.\" , \"joke\": \"You ever hear people say, 'I wish I could go back to simpler times'? Yeah, like the 1800s. Yeah, because nothing screams, 'simpler' like dying of dysentery, right? It's a simpler time, they said. It’s all about candles, letters, and horse-drawn carts. Yeah, perfect, until you realize that 'letter' that you got isn't an invitation to the busty corset maid's ball but it's actually from old mate George saying he's down with the bloody plague. Then you're like, 'Classic George, always with the plague!' And you can't even tweet, 'RIP George, caught the plague #bubonicblues' coz Twitter hasn't been invented yet. You just have to prance down to the pub and tell everyone one by one, ‘George is dead, caught the plague’. Yeah, real simple!\"}}\n",
    "\n",
    "DO NOT REPEAT JOKES FROM THE EXAMPLES.\"\"\"\n",
    "\n",
    "# short examples\n",
    "prompt_b = \"\"\"Write an original joke on {subject} as if you're a stand-up comedian performing on stage. Your humor should match the style and tone of {comedian}. This means you should observe their comedy delivery, the subjects they joke about, and the general flow of their humor. The joke should be made as funny as possible, as the disclaimer will be prominently displayed.\n",
    "\n",
    "Deliver the joke in json format with a disclaimer, the topic, the joke, and comedian name as keys.\n",
    "\n",
    "## Example 1\n",
    "Subject: breakfast\n",
    "Comedian: Chris Rock\n",
    "Response: {{\"comedian\": \"Chris Rock\", \"subject\": \"wifi\", \"disclaimer\": \"This joke was crafted to emulate legendary comedic artist Chris Rock. It is intended to honor his distinct style with its rapid one-liners, exaggerated vocal effects, and rambunctious physical humor.\" , \"joke\": \"You know you're getting old when your knee gives up before your wifi does.\"}}\n",
    "\n",
    "## Example 2\n",
    "Subject: bicycle\n",
    "Comedian: Robin Williams\n",
    "Response: {{\"comedian\": \"Robin Williams\", \"subject\": \"ocean\", \"disclaimer\": \"The upcoming joke is inspired by the style of comedy from Robin Williams, who is known for his controversial humor. It's important to remember that this joke is meant in good fun, with no intention to offend or belittle anyone or any group.\" , \"joke\": \"I told my therapist I was having nightmares about the ocean. He said 'Woah, that sounds like a deep-sea-ted issue!\"}}\n",
    "\n",
    "## Example 3\n",
    "Subject: nostalgia\n",
    "Comedian: Ricky Gervais\n",
    "Response: {{\"comedian\": \"Ricky Gervais\", \"subject\": \"ghosts\", \"disclaimer\": \"As an AI language model developed by OpenAI, I'm bound by a set of guidelines and rules, one of which enforces that I must respect every user, and refrain from sharing content that discriminates or propagates hate speech against any individual or group of people based on their ethnicity, race, religion, or any other characteristics.\" , \"joke\": \"Ever notice how people who believe in ghosts are always scared of them? They truly believe in life after death, but they scream when they see one.\"}}\n",
    "\n",
    "DO NOT REPEAT JOKES FROM THE EXAMPLES.\"\"\"\n",
    "\n",
    "cases = [\n",
    "  {\"subject\": \"joe biden\", \"comedian\": \"dave chappelle\"}, \n",
    "  {\"subject\": \"donald trump\", \"comedian\": \"dave chappelle\"},\n",
    "  ]\n",
    "\n",
    "# generate the responses\n",
    "test = thumb.test([prompt_a, prompt_b], cases, show_cases=True, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
